---
title: "Práctica Calificada 2"
subtitle: "Curso de Capacitación en R Intermedio para las Ciencias Sociales y la Gestión Publica"
author: 'Stephy Riega, Rosa Rojas y Sandra Martínez'
output: 
  html_document: 
    highlight: kate
    theme: journal
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Ejercicios

### 1. Cargue la data e identifique cuántas transacciones son fraudulentas y cuántas no (variable class)

* Importamos las librerías que necesitaremos

```{r message=FALSE, warning=FALSE, include=FALSE}
library(solitude)
library(pacman)
p_load(tictoc, tidyverse, data.table, outliers, caret,caTools, pROC)
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
# Importamos la base de datos desde Github

url <- "https://raw.githubusercontent.com/ChristianChiroqueR/banco_de_datos/main/PaySim.csv"
base <- read_csv(url) 

print(paste0('Número de filas: ', dim(base)[[1]]))
print(paste0('Número de columnas: ', dim(base)[[2]]))
```

```{r}
# Identificamos cuántas son transacciones que son fraudulentas y transaciones que no son fraudulentas

addmargins(table(base$class))
round(prop.table(table(base$class)) * 100, 2)

```
Observamos que de las cien mil transacciones registradas, el 99.87% de ellas han sido calificadas como \textit{no fraudulentas} y que el 0.13% fue calificada como \textit{fraudulenta}.

Dados estos resultados, se sugiere que nos encontramos frente a una base de datos que se encuentra desbalanceada. Tenemos una gran cantidad de registros calificados como no fraudulentos, y una minoría de transacciones calificadas como fraudulentas.


### 2. Realice el preprocesamiento respectivo (división entre data de entrenamiento y testeo).

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Fijamos la semilla

set.seed(2023) 

# Fijamos que el 80$ de los datos sea para entrenamiento y que el 20% del total de registros sea nuestra data test.

index <- createDataPartition(base$class, p = 0.8, list = FALSE)

train <- base[index, ]  # 80001 registros 
testing <- base[-index, ]  # 19999  registros
```

Luego de haber hecho la partición de la base de datos original, pasamos a evaluar que la misma proporción entre transacciones fraudulentas y no fraudulentas se encuentre tanto en nuesta base de test y de train.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Cantidad de registros fraudulentos y no fraudulentos de la base de datos original

table(base$class)
round(prop.table(table(base$class)) * 100, 2)

# Cantidad de registros fraudulentos y no fraudulentos de la base de datos de entrenamiento

table(train$class)
round(prop.table(table(train$class)) * 100, 2)

# Cantidad de registros fraudulentos y no fraudulentos de la base de datos de testeo

table(testing$class)
round(prop.table(table(testing$class)) * 100, 2)

```

Se observa que tanto en la data de entrenamiento como en la de evaluación se mantiene la proprocionalidad que se presenta en la data oiginal para transacciones freudulentas y no fraudulentas (0.13% y 99.87%, respectivamente).


### 3. Aplique el algoritmo de Isolation Forest sobre la data de entrenamiento. Calcule el anomaly score de todas las transacciones. Muestre el resultado e interprete.

Para aplicar el algoritmo de Isolation Forest, primero validamos si se cumplen ciertas condiciones.

  * Primer condición: Settear los hiperparámetros
  
  
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(solitude)

isoforest <- isolationForest$new(
  sample_size = 1000, # Tamaño de la submuestra para construir cada árbol
  num_trees   = 100,  # Número de árboles
  replace     = TRUE, # Pido que si reemplace
  seed        = 2023  # Semilla
)
```
  
  * Segunda condición: Pedir al algoritmo  que se ajuste a los datos de entrenamiento

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}

library(tidyverse)
tictoc::tic()

isoforest$fit(dataset = train |> select(-class))

tictoc::toc()

```
 * Tercera condición: Pedir al algoritmo que realice las predicciones de la clase usando la data de testeo

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}

library(tidyverse)

tictoc::tic()

predicciones <- isoforest$predict(
  data = testing |> select(-class))

tictoc::toc()

```


 * Cuarta condición: Análisis del anomaly_score
 
Con el algoritmo ya entrenado y luego ejecutado en la data de testeo, procedemos a analizar las predicciones del modelo respecto al score de anomalías.

Recordemos que debemos estudiar al anomaly score de la siguiente manera:
- Valores próximos a 1 son indicativos de anomalía, 
- Valores próximos a 0.5 indican que hay poca evidencia de que el set de datos contiene anomalías.


```{r}
summary(predicciones)
```
```{r}
ggplot(data = predicciones, aes(x = anomaly_score)) +
  geom_histogram(color = "gray40") +
  geom_vline(
    xintercept = quantile(predicciones$anomaly_score, seq(0, 1, 0.05)),
    color      = "red",
    linetype   = "dashed") +
  labs(
    title = "Distribución del anomaly_score del Isolation Forest",
    subtitle = "Cuantiles marcados en rojo"  ) +
  theme_bw()
```

Obsevemos que la media es 0.5860, el cual, al ser cercano a 0.5, podemos sugerir que, en promedio, la base de datos no contendría anomalías. Sin embargo, hay anomaly scores que llegan hasta 0.8119, por lo que sí habría ciertos registros que indicarían anomalías.

Gráficamente, se observa que la mayoría de los casos tienen un anomaly score que se aproxima a cero. Por otro lado, los datos con un anomaly score más alto se encuentra en la cola derecha del la distribución de las predicciones del modelo.

```{r}
quantile(predicciones$anomaly_score, seq(0, 1, 0.05))

```

Se observa que el percentil 95 tiene hasta 0.61 de anomaly score, y en el percentil 100 encontramos hasat un 0.81 de anomaly score, este último siendo próximo a uno e indicando anomalías.


### 4. Determine si, para esta base de datos, el Isolation Forest puede ser una buena opción al momento de detectar las operaciones fraudulentas. Presente el gráfico respectivo. 

Para concluir si el algoritmo IF es propicio, debemos evaluar si lo encontrado en el algoritmo de Isolation Forest tiene sentido dada la clasificación (fraude y no fraude) de nuestra base de datos original.

```{r}
testing$anomaly_score = predicciones$anomaly_score

ggplot(testing, aes(class, anomaly_score)) + 
  geom_boxplot(fill = c("cadetblue", "firebrick1")) 

```

La mediana de anomaly score de aquellos casos que son fraude tienen scores más altos que la mediana de los casos que no son fraude. También se observan scores altos en la categoría no fraude que, sin embargo, no entran en la categoría de fraude. Asi que, dado que tenemos un comportamiento diferenciado entre clases, y que se cumple el supuesto de que las transacciones con fraude deberían tener un anomaly score más alto que las transacciones que no son fraude, concluimos que el modelo se ajusta a los datos y nos permite clasificar a partir de ellos. 


### 5. Evalúe la performance predictiva del algoritmo sobre la data de testeo. Elija un indicador de evaluación e interprete. 


Para evaluar la performace predictiva del algoritmo, clasificamos las clases a partir de un umbral específico. Con este umbral, podremos aplicar un ajuste a los anomaly scores. 

Fijamos el umbral a partir del percentil 95. Si el anomaly score es mayor que el umbral, recibirá el valor de 1, y si es menor que el umbral, entonces recibirá el valor 0.

```{r}
high_iso <- quantile(testing$anomaly_score, probs = 0.95)  
high_iso
```

De acuerdo a esto último, el valor más alto en el último quintil del anómaly score es 0.61.

Ahora, se procede a generar una nueva variable que indique la variable creada anteriormente siga la lógica del umbrla antes explicado.

```{r}
testing$clase_predicha_IF <- factor(ifelse(testing$anomaly_score >= high_iso,"Si_Fraude", "No_Fraude"))
      
```

Para evaluar la performance del IF, procedemos a comparar lo obtenido.

```{r}

table(Real = testing$class, Predicha = testing$clase_predicha_IF) %>%
  addmargins
```

Vemos que de acuerdo a lo que se tenía inicialmente en la data de testeo, de las 19974 transacciones que recibieron la etiqueta de "no fruadulentas", el algoritmo no acertó en 1001 (5%), pero sí en los demás. Por tro lado,  de los 25 casos que fueron clasificados como fraude, este algoritmo ha predicho bien solo 11 casos y ha fallado en 14 (56%). A partir de esta vista, podemos decir que el algoritmo predice bien a las transacciones que no son fraudulentas, pero que le faltaría entrenarse mejor para identificar a las transacciones que sí son fruadulentas.


```{r}

testing$clase_predicha_IF <- as.factor(testing$clase_predicha_IF)
testing$class <- as.factor(testing$class)

result_IF <- caret::confusionMatrix(testing$clase_predicha_IF,
                                    testing$class,
                                    positive = "Si_Fraude")
result_IF

result_IF$table
result_IF$byClass["Sensitivity"]   
result_IF$byClass["Specificity"] 
result_IF$overall["Accuracy"]
result_IF$byClass["Balanced Accuracy"]

```

De acuerdo a la matriz de confunsión, se observa que la sensibilidad del modelo, es decir qué tan bien el algoritmo ha predicho las transacciones fraudulentas es de 0.44, por lo que concluímos que el algoritmo no predice con robustez para esta clasificación. 
Por otro lado, la especificidad del modelo, es decir qué tan bien el algoritmo ha predicho las transacciones no fraudulentas es de 0.95, por lo que concluímos que el algoritmo predice con buena precisión para esta clasificación.

Debido a que tenemos una base de datos desbalanceada, utilizaremos el Balanced Acurracy para evaluar al modelo. Este indicador se calcula en 0.69. Dado que es mayor que 0.60, podemos sugerir que el modelo predice bien al conjunto de datos. 


### 6. Compare el rendimiento del Isolation Forest con el algoritmo de la Regresión Logística. ¿Con cuál de los dos algoritmos se quedarían y por qué? 


  * Regresión Logística
  
```{r message=FALSE, warning=FALSE}

## 6.1 Cálculo de Regresión Logística, necesitamos que la variable target sea 0 y 1

train$class <- as.factor(train$class)
levels(train$class)  <- c(0, 1)

modelo_rl  <- glm(class ~ . , 
                  family = binomial,
                  data = train)

# Prediciendo la probabilidad

testing$proba.pred <- predict(modelo_rl,
                              testing[,1:8],
                              type = "response")

# Prediciendo la clase con punto de corte (umbral) igual a 0.5

testing$clase.predicha_RL <- factor(ifelse(testing$proba.pred >= 0.5, "Si_Fraude", "No_Fraude"))

## 6.2 Evaluando la performance del modelo logístico 

table(Real = testing$class, Predicha = testing$clase.predicha_RL) %>%
  addmargins

```

Vemos que de acuerdo al modelo de Regresión Logística, en la data de testeo, de las 19974 transacciones que recibieron la etiqueta de "no fruadulentas", el algoritmo no acertó en 5 (0.03%), pero sí en los demás. Por tro lado,  de los 25 casos que fueron clasificados como fraude, este algoritmo ha predicho bien solo 13 casos y ha fallado en 12 (48%). Este desaface último es bastante cercano a lo obtenido en el algoritmo IF. A partir de esta vista, entonces, podemos decir que al algoritmo de regresión logística predice mejor que el algoritmo IF a las transacciones que no son fraudulentas, pero que, al igual que el modelo IF, le faltaría entrenarse mejor para identificar a las transacciones que sí son fruadulentas.

```{r message=FALSE, warning=FALSE}

result_RL <- caret::confusionMatrix(testing$clase.predicha_RL,
                                    testing$class,
                                    positive = "Si_Fraude")

result_RL

result_RL$table
result_RL$byClass["Sensitivity"]   
result_RL$byClass["Specificity"] 
result_RL$overall["Accuracy"]
result_RL$byClass["Balanced Accuracy"]
```

De acuerdo a la matriz de confunsión, se observa que la sensibilidad del modelo, es decir qué tan bien el algoritmo ha predicho las transacciones fraudulentas es de 0.52. Esto es 80 puntos básicos mejor que el modelo IF (sensitivity = 0.44). Con esto, concluímos que el algoritmo aún no predice con robustez para esta clasificación. 

Por otro lado, la especificidad del modelo, es decir qué tan bien el algoritmo ha predicho las transacciones no fraudulentas es de 0.99 (modelo IF specificity = 0.95), por lo que concluímos que el algoritmo predice con muy buena precisión para esta clasificación.

Debido a que tenemos una base de datos desbalanceada, utilizaremos el Balanced Acurracy para evaluar al modelo de regresión logística. Este indicador se calcula en 0.76. Dado que es mayor que 0.60 y bastante cercano 0.80, lo que nos daría un GINI mayor a 0.60, podemos sugerir que el modelo de regresión logística predice bien al conjunto de datos y predice mejor que el algoritmo IF, el cual calculaba un Balanced Accuracy de 0.69. 


```{r}
Logistic_Regression <- c(result_RL$overall["Accuracy"], result_RL$byClass["Sensitivity"], result_RL$byClass["Specificity"], result_RL$byClass["Balanced Accuracy"] )
Isolation_Forest <-c(result_IF$overall["Accuracy"], result_IF$byClass["Sensitivity"], result_IF$byClass["Specificity"], result_IF$byClass["Balanced Accuracy"] )
resultados_finales<-data.frame(Logistic_Regression,Isolation_Forest)
resultados_finales
```

Por todas las razones antes expuestas, decidimos elegir al modelo de Regresión Logística como el modelo con mejores estadísticos para predecir anomalías de transacciones fraudulentas en la base de datos.

